# The content of the this file will be used to fill out the cluster profile.
# This is the Cluster configuration designed for on-cluster use
# Comment any fields not required or leave them empty.

# Copyright 2022-2025 The MathWorks, Inc.

########################################   REQUIRED   ########################################

# Cluster Name

Name = Explorer


# MATLAB Parallel Server Worker Count
# Number must not exceed the number of MATLAB Parallel Server licenses purchased.

NumWorkers = 1000


# Specify if the client and the cluster are able to access the same network drive/filesystem.
# Default value for the Cluster config is true

HasSharedFilesystem = true


# Specify Cluster Operating System

OperatingSystem = unix


# Default PluginScriptsLocation.  For example:
# PluginScriptsLocation = /opt/apps/MATLAB/support_packages/matlab_parallel_server/scripts/IntegrationScripts/explorer

PluginScriptsLocation = /shared/EL9/explorer/matlab/support_packages/matlab_parallel_server/scripts/IntegrationScripts/explorer


# Default job storage location
# Location on the cluster where job data should be stored

JobStorageLocation = "$HOME"/.matlab/generic_cluster_jobs/explorer


##################################   ADDITIONAL PROPERTIES   #################################

[AdditionalProperties]


#######################
# Required Properties #
#######################

# Login/Head node that MATLAB submits jobs to

ClusterHost = login.explorer.northeastern.edu
MirrorHost = xfer.discovery.neu.edu

# Remote Job Storage Location
# Directory on the cluster's file system to be used as the remote job storage location.

RemoteJobStorageLocation (Windows) = /home/"$USERNAME"/.matlab/generic_cluster_jobs/discovery/"$COMPUTERNAME"
RemoteJobStorageLocation (Unix) = /home/"$USER"/.matlab/generic_cluster_jobs/discovery/"$HOST"


#######################
# Optional Properties #
#######################

AccountName = 
AdditionalSubmitArgs = 
Constraint = 
DisplaySubmitArgs = true
EmailAddress = 
EnableDebug = false
GPUCard = 
GPUsPerNode = 0
MemPerCPU = 4gb
#MPIImplementation = IntelMPI
Partition = 
ProcsPerNode = 0
RequireExclusiveNode = false
Reservation = 
#UseBindToCore = false
WallTime = 
